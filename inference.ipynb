{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import zarr\n",
    "import os \n",
    "from Compressor import Focalizer\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "# Specify device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "input_tensor = torch.load('/home/roberto/PythonProjects/SSFocus/Data/4096_test_fft2D.pt').to(device)\n",
    "\n",
    "# Usage example\n",
    "# file_path = \"Mini_R2F.zarr\"\n",
    "\n",
    "# # To read the root array or group\n",
    "# root = read_zarr_file(file_path)\n",
    "\n",
    "# # To read a specific array or group\n",
    "# raw = read_zarr_file(file_path, \"raw\")\n",
    "# gt = read_zarr_file(file_path, \"gt\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the version: version_8\n",
      "Len tx replica: 4549\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (227) must match the existing size (4549) at non-singleton dimension 0.  Target sizes: [227].  Tensor sizes: [4549]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/roberto/PythonProjects/SSFocus/inference.ipynb Cella 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Besago/home/roberto/PythonProjects/SSFocus/inference.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m aux \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39m/home/roberto/PythonProjects/SSFocus/Data/RAW/SM/numpy/s1a-s1-raw-s-vv-20200509t183227-20200509t183238-032492-03c34a_pkt_8_metadata.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Besago/home/roberto/PythonProjects/SSFocus/inference.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m eph \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39m/home/roberto/PythonProjects/SSFocus/Data/RAW/SM/numpy/s1a-s1-raw-s-vv-20200509t183227-20200509t183238-032492-03c34a_ephemeris.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Besago/home/roberto/PythonProjects/SSFocus/inference.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m Focalizer(metadata\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39maux\u001b[39;49m\u001b[39m'\u001b[39;49m:aux, \u001b[39m'\u001b[39;49m\u001b[39mephemeris\u001b[39;49m\u001b[39m'\u001b[39;49m:eph})\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Besago/home/roberto/PythonProjects/SSFocus/inference.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# If your model was wrapped with a LightningModule, you may need to prepend 'model.' to each key\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Besago/home/roberto/PythonProjects/SSFocus/inference.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict({key\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mmodel.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m): value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m checkpoint[\u001b[39m'\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/PythonProjects/SSFocus/Compressor.py:67\u001b[0m, in \u001b[0;36mFocalizer.__init__\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39maux\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m load_constants_from_meta(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeta)\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_velocity(metadata) \n\u001b[1;32m     69\u001b[0m \u001b[39m# slant range vector:\u001b[39;00m\n",
      "File \u001b[0;32m~/PythonProjects/SSFocus/constants.py:43\u001b[0m, in \u001b[0;36mload_constants_from_meta\u001b[0;34m(meta, patch_dim)\u001b[0m\n\u001b[1;32m     40\u001b[0m index_start \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mceil(torch\u001b[39m.\u001b[39mtensor((constants[\u001b[39m'\u001b[39m\u001b[39mlen_az_line\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39mnum_tx_vals)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     41\u001b[0m index_end \u001b[39m=\u001b[39m num_tx_vals \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mceil(torch\u001b[39m.\u001b[39mtensor((constants[\u001b[39m'\u001b[39m\u001b[39mlen_az_line\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39mnum_tx_vals)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m---> 43\u001b[0m range_filter[\u001b[39mint\u001b[39;49m(index_start):\u001b[39mint\u001b[39;49m(index_end\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)] \u001b[39m=\u001b[39m tx_replica\n\u001b[1;32m     44\u001b[0m constants[\u001b[39m'\u001b[39m\u001b[39mrange_filter\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconj(torch\u001b[39m.\u001b[39mfft\u001b[39m.\u001b[39mfft(range_filter))\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m constants\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (227) must match the existing size (4549) at non-singleton dimension 0.  Target sizes: [227].  Tensor sizes: [4549]"
     ]
    }
   ],
   "source": [
    "# Load model and weights\n",
    "folders = [x.stem.split('_')[-1] for x in Path('/home/roberto/PythonProjects/SSFocus/lightning_logs').iterdir()]\n",
    "idx = max([int(x) for x in folders])\n",
    "version = f'version_{idx}'\n",
    "print('Loaded the version:', version)\n",
    "\n",
    "checkpoint_file = find_checkpoint(f\"/home/roberto/PythonProjects/SSFocus/lightning_logs/{version}/\")\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "\n",
    "aux = pd.read_pickle('/home/roberto/PythonProjects/SSFocus/Data/RAW/SM/numpy/s1a-s1-raw-s-vv-20200509t183227-20200509t183238-032492-03c34a_pkt_8_metadata.pkl')\n",
    "eph = pd.read_pickle('/home/roberto/PythonProjects/SSFocus/Data/RAW/SM/numpy/s1a-s1-raw-s-vv-20200509t183227-20200509t183238-032492-03c34a_ephemeris.pkl')\n",
    "model = Focalizer(metadata={'aux':aux, 'ephemeris':eph}).to(device)\n",
    "\n",
    "# If your model was wrapped with a LightningModule, you may need to prepend 'model.' to each key\n",
    "model.load_state_dict({key.replace('model.', ''): value for key, value in checkpoint['state_dict'].items()})\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor.unsqueeze(0).unsqueeze(0))  # Adding a batch dimension\n",
    "    output_tensor = output_tensor.squeeze(0).squeeze(0)\n",
    "    output_tensor = output_tensor.cpu().numpy()  # Removing batch dimension and sending to cpu\n",
    "# model._plot_tensor(input_tensor)\n",
    "model._plot_tensor(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model._plot_rg_filter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Focusing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
