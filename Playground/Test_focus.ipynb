{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sentinel1decoder\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from SARProcessor.focus import RD, get_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swath numbers: [12 93 10 11 43]\n",
      "filename:  s1a-iw-raw-s-vh-20200102t162318-20200102t162351-030624-038242\n"
     ]
    }
   ],
   "source": [
    "inputfile = '/home/roberto/PythonProjects/SSFocus/Data/RAW/IW/ATENE/S1A_IW_RAW__0SDV_20200102T162318_20200102T162351_030624_038242_4CBE.SAFE/s1a-iw-raw-s-vh-20200102t162318-20200102t162351-030624-038242.dat'\n",
    "filename = Path(inputfile).stem\n",
    "decoder = sentinel1decoder.Level0Decoder(inputfile, log_level=logging.WARNING)\n",
    "raw_df = decoder.decode_metadata()\n",
    "ephemeris = sentinel1decoder.utilities.read_subcommed_data(raw_df)\n",
    "print(\"Swath numbers:\",raw_df['Swath Number'].unique())\n",
    "print('filename: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Packet Version Number', 'Packet Type', 'Secondary Header Flag', 'PID',\n",
      "       'PCAT', 'Sequence Flags', 'Packet Sequence Count', 'Packet Data Length',\n",
      "       'Coarse Time', 'Fine Time', 'Sync', 'Data Take ID', 'ECC Number',\n",
      "       'Test Mode', 'Rx Channel ID', 'Instrument Configuration ID',\n",
      "       'Sub-commutated Ancilliary Data Word Index',\n",
      "       'Sub-commutated Ancilliary Data Word', 'Space Packet Count',\n",
      "       'PRI Count', 'Error Flag', 'BAQ Mode', 'BAQ Block Length',\n",
      "       'Range Decimation', 'Rx Gain', 'Tx Ramp Rate',\n",
      "       'Tx Pulse Start Frequency', 'Tx Pulse Length', 'Rank', 'PRI', 'SWST',\n",
      "       'SWL', 'SAS SSB Flag', 'Polarisation', 'Temperature Compensation',\n",
      "       'Calibration Mode', 'Tx Pulse Number', 'Signal Type', 'Swap Flag',\n",
      "       'Swath Number', 'Number of Quads'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols = raw_df.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(cols)):\n",
    "#     if cols[idx] == 'Signal Type':\n",
    "#         print(idx, cols[idx], raw_df[cols[idx]].unique())\n",
    "        \n",
    "# Writing the dictionary to parse the calibration data as:\n",
    "        # O: echo\n",
    "        # 1: Noise\n",
    "        # 2 to 7: NotApplicable\n",
    "        # 13 to 14: NotApplicable\n",
    "        # 12: APDNcal\n",
    "        # 8: Tx Cal\n",
    "        # 15: TxHCaliso\n",
    "        # 9: Rx Cal\n",
    "        # 10: EPDN Cal\n",
    "        # 11: TA Cal\n",
    "cal_dict =  {0: 'echo', 1: 'noise', 2: 'notapplicable', 3: 'notapplicable', 4: 'notapplicable', 5: 'notapplicable', 6: 'notapplicable', 7: 'notapplicable', 8: 'txcal', 9: 'rxtcal', 10: 'epdncal', 11: 'tacal', 12: 'apdncal', 13: 'notapplicable', 14: 'notapplicable', 15: 'txhcaliso'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['calibration'] = raw_df['Signal Type'].map(cal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo         51226\n",
      "apdncal        240\n",
      "rxtcal         120\n",
      "epdncal        120\n",
      "tacal          120\n",
      "txcal           72\n",
      "txhcaliso       48\n",
      "Name: calibration, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# explore how many rows are there for each calibration type:\n",
    "print(raw_df['calibration'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  129,   130,   131,   132,   133,   134,   135,   136,   137,\n",
      "              138,\n",
      "            ...\n",
      "            51833, 51834, 51835, 51836, 51837, 51838, 51839, 51840, 51841,\n",
      "            51842],\n",
      "           dtype='int64', length=720)\n"
     ]
    }
   ],
   "source": [
    "# explore how many rows are there for each calibration type:\n",
    "# print(raw_df['calibration'].value_counts())\n",
    "\n",
    "# find the indexes when raw_df['calibration'] passes from echo to other:\n",
    "echo_to_other = raw_df[raw_df['calibration'] != 'echo'].index\n",
    "print(echo_to_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['echo' 'apdncal' 'txcal' 'txhcaliso' 'rxtcal' 'epdncal' 'tacal']\n",
      "apdncal:  240\n",
      "txcal:  72\n",
      "txhcaliso:  48\n",
      "rxtcal:  120\n",
      "epdncal:  120\n",
      "tacal:  120\n"
     ]
    }
   ],
   "source": [
    "print(raw_df['calibration'].unique())\n",
    "\n",
    "# Extracting the calibration data: 'apdncal' 'txcal' 'txhcaliso' 'rxtcal' 'epdncal' 'tacal'\n",
    "apdncal = raw_df[raw_df['calibration'] == 'apdncal']\n",
    "txcal = raw_df[raw_df['calibration'] == 'txcal']\n",
    "txhcaliso = raw_df[raw_df['calibration'] == 'txhcaliso']\n",
    "rxtcal = raw_df[raw_df['calibration'] == 'rxtcal']\n",
    "epdncal = raw_df[raw_df['calibration'] == 'epdncal']\n",
    "tacal = raw_df[raw_df['calibration'] == 'tacal']\n",
    "\n",
    "# printing the dimensions of the calibration data:\n",
    "print('apdncal: ', apdncal.shape[0])\n",
    "print('txcal: ', txcal.shape[0])\n",
    "print('txhcaliso: ', txhcaliso.shape[0])\n",
    "print('rxtcal: ', rxtcal.shape[0])\n",
    "print('epdncal: ', epdncal.shape[0])\n",
    "print('tacal: ', tacal.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a multi-plot of all the columns in the dataframe:\n",
    "tmp = raw_df\n",
    "tmp.plot(subplots=True, figsize=(15, 60))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "raw_df[\"Swath Number\"].plot()\n",
    "plt.ylim([8,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chunks = get_chunks(raw_df)\n",
    "except:\n",
    "    print(f\"The chunking failed for {inputfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, chunk in enumerate(chunks):\n",
    "    focuser = RD(decoder=decoder, raw=chunk, ephemeris=ephemeris)\n",
    "    img_focused = focuser.process()\n",
    "    pd.to_pickle(img_focused, f\"/home/roberto/PythonProjects/SSFocus/Data/RAW/IW/ROME/Processed/{idx}_chunk_{filename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/roberto/PythonProjects/SSFocus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the focus of a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from SARProcessor.focus import plot_img_focused\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subswaths = {}\n",
    "for i in range(32):\n",
    "    subswaths[i] = pd.read_pickle(f'/home/roberto/PythonProjects/SSFocus/Data/FOCUSED/IW/ATENE/{i}_subswath.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(range(1,32,3))\n",
    "B = list(range(2,32,3))\n",
    "C = list(range(3,32,3))\n",
    "\n",
    "img_stacked_A, img_stacked_B, img_stacked_C = [], [], []\n",
    "for j in range(0,len(A)-1,2):\n",
    "    img_stacked_A.append(np.hstack((subswaths[A[j]],subswaths[A[j+1]])))\n",
    "    \n",
    "# for j in range(0,len(B),2):\n",
    "#     img_stacked_B.append(np.hstack((subswaths[B[j]],subswaths[B[j+1]])))\n",
    "    \n",
    "# for j in range(0, len(C)-1,2):\n",
    "#     img_stacked_C.append(np.hstack((subswaths[C[j]],subswaths[C[j+1]])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"figsize\": (50, 50),\n",
    "    \"vmin\": 80,\n",
    "    \"vmax\": 400,\n",
    "    \"cmap\": \"gray\",\n",
    "    \"showAxis\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_stack = np.vstack(img_stacked_A[:3])\n",
    "\n",
    "plot_img_focused(im_stack, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_focused(img_stacked_B[0], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_stacked_B[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.hstack(subswaths[1], subswaths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(0,28,3):\n",
    "    print(subswaths[i].shape)\n",
    "    plot_img_focused(subswaths[i], **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IW1 = list(range(0,28,3))\n",
    "\n",
    "img_stacked = []\n",
    "for j in range(1,len(IW1)-1,2):\n",
    "    print(j)\n",
    "    img_stacked.append(np.hstack((subswaths[IW1[j]],subswaths[IW1[j+1]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_v_stacked = np.vstack(img_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"figsize\": (150, 150),\n",
    "    \"vmin\": 50,\n",
    "    \"vmax\": 700,\n",
    "    \"cmap\": \"gray\",\n",
    "    \"showAxis\": False,\n",
    "}\n",
    "\n",
    "plot_img_focused(img_v_stacked, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"s1a-iw-raw-s-vv-20201203t052021-20201203t052053-035517-042709\"\n",
    "basepath = f'{os.environ[\"SARLENS_DIR\"]}/Data/FOCUSED/IW/ROME/{filename}'\n",
    "chunks = [x for x in Path(basepath).glob('*.pkl')]\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "assert len(chunks) > 0, \"No chunks found!\"\n",
    "\n",
    "def check_shapes(IW):\n",
    "    shape_0 = IW[0].shape\n",
    "    for ss in IW:\n",
    "        assert ss.shape == shape_0, \"Shapes are not the same!\"\n",
    "        \n",
    "\n",
    "# check the unique shapes of the chunks\n",
    "shapes = []\n",
    "for ss in chunks:\n",
    "    ii = pd.read_pickle(ss)\n",
    "    shape_i = ii.shape[1]\n",
    "    shapes.append(shape_i)\n",
    "\n",
    "unique = np.unique(shapes, axis=0)\n",
    "print(unique)\n",
    "\n",
    "for chunk_n in range(len(chunks)):\n",
    "    img = pd.read_pickle(basepath + f\"/{chunk_n}_subswath_{filename}.pkl\")\n",
    "    print(f\"Chunk n. {chunk_n} with shape:\", img.shape)\n",
    "    \n",
    "\n",
    "# IW1, IW2, IW3 = [], [], []\n",
    "\n",
    "# for chunk_n in range(0, len(chunks), 3):\n",
    "#     img = pd.read_pickle(basepath + f\"/{chunk_n}_subswath_{filename}.pkl\")\n",
    "#     print(f\"Chunk n. {chunk_n} with shape:\", img.shape)\n",
    "#     IW1.append(img)\n",
    "# check_shapes(IW1)\n",
    "    \n",
    "# for chunk_n in range(1, len(chunks), 3):\n",
    "#     img = pd.read_pickle(basepath + f\"/{chunk_n}_subswath_{filename}.pkl\")\n",
    "#     print(f\"Chunk n. {chunk_n} with shape:\", img.shape)\n",
    "#     IW2.append(img)\n",
    "# check_shapes(IW2)\n",
    "\n",
    "# for chunk_n in range(2, len(chunks), 3):\n",
    "#     img = pd.read_pickle(basepath + f\"/{chunk_n}_subswath_{filename}.pkl\")\n",
    "#     print(f\"Chunk n. {chunk_n} with shape:\", img.shape)\n",
    "#     IW3.append(img)\n",
    "# check_shapes(IW3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the images along the rows:\n",
    "IW1_stacked = np.vstack(IW1)\n",
    "IW2_stacked = np.vstack(IW2)    \n",
    "IW3_stacked = np.vstack(IW3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"figsize\": (20, 20),\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 1500,\n",
    "    \"cmap\": \"gray\",\n",
    "    \"showAxis\": False,\n",
    "}\n",
    "\n",
    "plot_img_focused(stackedImage, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAQ (Binary Amplitude Quantization) mode is a specific data quantization technique used in Synthetic Aperture Radar (SAR) systems to compress and reduce the amount of data generated during the radar imaging process. SAR systems produce high-resolution images of the Earth's surface by transmitting radar signals and then processing the reflected signals (echoes) from the ground. This process generates a large volume of data, which can be challenging to manage, store, and transmit.\n",
    "\n",
    "BAQ mode reduces the amount of data by quantizing the amplitude of the received radar echoes. This is achieved by converting the continuous amplitude values of the radar echoes into discrete levels or \"bins.\" Each amplitude value is then represented by a binary code, which requires fewer bits than the original continuous amplitude value.\n",
    "\n",
    "The main advantage of using BAQ mode in SAR systems is the significant reduction in data size, which simplifies data handling, storage, and transmission. However, quantization can introduce some loss of information, which may affect the image quality to a certain extent. The trade-off between data reduction and image quality depends on the number of quantization levels (or bits) used. More quantization levels result in better image quality but lower data reduction, whereas fewer quantization levels yield higher data reduction but potentially lower image quality.\n",
    "\n",
    "In summary, BAQ mode is a data quantization technique used in SAR systems to reduce the amount of data generated during the radar imaging process. This technique allows for more manageable data handling, storage, and transmission at the expense of some loss in image quality, depending on the number of quantization levels used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df[\"BAQ Mode\"].to_list())\n",
    "plt.xlim([0,1000])\n",
    "plt.title('BAQ')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = SARLens(decoder, selection, ephemeris)\n",
    "# processor.decode_file()\n",
    "# processor.extract_parameters()\n",
    "# processor.calculate_wavelength()\n",
    "# processor.calculate_sample_rates()\n",
    "# processor.create_fast_time_vector()\n",
    "# processor.calculate_slant_range()\n",
    "# processor.calculate_axes()\n",
    "# processor.calculate_spacecraft_velocity()\n",
    "# processor.calculate_positions()\n",
    "# processor.calculate_velocity_and_d()\n",
    "# processor.process_freq_domain_data()\n",
    "# processor.apply_range_filter()\n",
    "# processor.apply_rcmc_filter()\n",
    "# processor.apply_azimuth_filter()\n",
    "# processor.plot_img()\n",
    "\n",
    "# # processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SARLens(decoder, selection, ephemeris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print('Decode File:')\n",
    "t0 = time.time()\n",
    "processor.decode_file()\n",
    "t1 = time.time()\n",
    "print('Execution time:', t1 - t0)\n",
    "\n",
    "print('extract_parameters:')\n",
    "t2 = time.time()\n",
    "processor.extract_parameters()\n",
    "t3 = time.time()\n",
    "print('Execution time:', t3 - t2)\n",
    "\n",
    "print('calculate_wavelength:')\n",
    "t4 = time.time()\n",
    "processor.calculate_wavelength()\n",
    "t5 = time.time()\n",
    "print('Execution time:', t5 - t4)\n",
    "\n",
    "print('calculate_sample_rates:')\n",
    "t6 = time.time()\n",
    "processor.calculate_sample_rates()\n",
    "t7 = time.time()\n",
    "print('Execution time:', t7 - t6)\n",
    "\n",
    "print('create_fast_time_vector:')\n",
    "t8 = time.time()\n",
    "processor.create_fast_time_vector()\n",
    "t9 = time.time()\n",
    "print('Execution time:', t9 - t8)\n",
    "\n",
    "print('calculate_slant_range:')\n",
    "t10 = time.time()\n",
    "processor.calculate_slant_range()\n",
    "t11 = time.time()\n",
    "print('Execution time:', t11 - t10)\n",
    "\n",
    "print('calculate_axes:')\n",
    "t12 = time.time()\n",
    "processor.calculate_axes()\n",
    "t13 = time.time()\n",
    "print('Execution time:', t13 - t12)\n",
    "\n",
    "print('calculate_spacecraft_velocity:')\n",
    "t14 = time.time()\n",
    "processor.calculate_spacecraft_velocity()\n",
    "t15 = time.time()\n",
    "print('Execution time:', t15 - t14)\n",
    "\n",
    "print('calculate_positions:')\n",
    "t16 = time.time()\n",
    "processor.calculate_positions()\n",
    "t17 = time.time()\n",
    "print('Execution time:', t17 - t16)\n",
    "\n",
    "print('calculate_velocity_and_d:')\n",
    "t18 = time.time()\n",
    "processor.calculate_velocity_and_d()\n",
    "t19 = time.time()\n",
    "print('Execution time:', t19 - t18)\n",
    "\n",
    "print('process_freq_domain_data:')\n",
    "t20 = time.time()\n",
    "processor.process_freq_domain_data()\n",
    "t21 = time.time()\n",
    "print('Execution time:', t21 - t20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('apply_range_filter:')\n",
    "t0 = time.time()\n",
    "processor.apply_range_filter()\n",
    "t1 = time.time()\n",
    "print('Execution time:', t1 - t0)\n",
    "\n",
    "print('apply_rcmc_filter:')\n",
    "t2 = time.time()\n",
    "processor.apply_rcmc_filter()\n",
    "t3 = time.time()\n",
    "print('Execution time:', t3 - t2)\n",
    "\n",
    "# print('apply_azimuth_filter:')\n",
    "# t4 = time.time()\n",
    "# processor.apply_azimuth_filter()\n",
    "# t5 = time.time()\n",
    "# print('Execution time:', t5 - t4)\n",
    "\n",
    "# print('plot_img:')\n",
    "# t6 = time.time()\n",
    "# processor.plot_img()\n",
    "# t7 = time.time()\n",
    "# print('Execution time:', t7 - t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,15))\n",
    "plt.imshow(abs(processor.range_doppler_data))\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.len_az_line\n",
    "\n",
    "\n",
    "\"\"\"Apply the azimuth filter and create the compressed data.\"\"\"\n",
    "processor.az_compressed_data = np.zeros((processor.len_az_line, processor.len_range_line), 'complex')\n",
    "\n",
    "for az_line_index in range(processor.len_range_line):\n",
    "       try:\n",
    "              # d_vector = np.zeros(processor.len_az_line)\n",
    "\n",
    "              this_az_filter = np.zeros(processor.len_az_line, 'complex')\n",
    "              # for ii in range(len(processor.az_freq_vals) - 1):  # -1\n",
    "              for ii in range(len(processor.az_freq_vals) - 1):  # -1\n",
    "                     this_az_filter[ii] = cmath.exp((4j * cmath.pi * processor.slant_range[ii] * processor.D[ii, az_line_index]) / processor.wavelength)\n",
    "       \n",
    "       except IndexError:\n",
    "              print('Error')\n",
    "              print('len_processor.az_freq_vals:',len(processor.az_freq_vals))\n",
    "              print('i:',ii)\n",
    "              print('slant_range.shape:',processor.slant_range.shape)\n",
    "              print('slant_range.shape:',processor.D.shape)\n",
    "              print('az_line_index:',az_line_index)\n",
    "\n",
    "       result = processor.range_doppler_data[:, az_line_index] * this_az_filter[:]\n",
    "       result = np.fft.ifft(result)\n",
    "       processor.az_compressed_data[:, az_line_index] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(abs(processor.az_compressed_data), vmin=0, vmax=1000, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE CODE\n",
    "\n",
    "iq_array = decoder.decode_file(selection)\n",
    "print(\"Raw data shape: \", iq_array.shape)\n",
    "# Image sizes\n",
    "len_range_line = iq_array.shape[1]\n",
    "len_az_line = iq_array.shape[0]\n",
    "\n",
    "# Extract necessary parameters, see both reference documents.\n",
    "# All necessary parameters are included into the dataframe\n",
    "c = sentinel1decoder.constants.speed_of_light\n",
    "TXPL = selection[\"Tx Pulse Length\"].unique()[0]\n",
    "print(\"Tx Pulse length\", TXPL)\n",
    "TXPSF = selection[\"Tx Pulse Start Frequency\"].unique()[0]\n",
    "print(\"Tx Pulse Start Freq\", TXPSF)\n",
    "TXPRR = selection[\"Tx Ramp Rate\"].unique()[0]\n",
    "print(\"Tx Ramp Rate\", TXPRR)\n",
    "RGDEC = selection[\"Range Decimation\"].unique()[0]\n",
    "print(\"Range Decimation\", RGDEC)\n",
    "PRI = selection[\"PRI\"].unique()[0]\n",
    "print(\"PRI\", RGDEC)\n",
    "rank = selection[\"Rank\"].unique()[0]\n",
    "print(\"Rank\", rank)\n",
    "suppressed_data_time = 320/(8*sentinel1decoder.constants.f_ref)  # see pag. 82 of the reference document (Airbus)\n",
    "print(suppressed_data_time)\n",
    "range_start_time = selection[\"SWST\"].unique()[0] + suppressed_data_time\n",
    "wavelength = c / 5.405e9\n",
    "\n",
    "# Sample rates\n",
    "range_sample_freq = sentinel1decoder.utilities.range_dec_to_sample_rate(RGDEC)\n",
    "range_sample_period = 1 / range_sample_freq\n",
    "az_sample_freq = 1 / PRI\n",
    "az_sample_period = PRI\n",
    "\n",
    "# Fast time vector [s] - defines the time axis along the fast time direction\n",
    "range_line_num = [i for i in range(len_range_line)]\n",
    "fast_time = [range_start_time + i * range_sample_period for i in range_line_num]\n",
    "\n",
    "\n",
    "# Slant range vector - defines R0, the range of the closest approach for each range cell (i.e. the slant range when\n",
    "# the radar is closest to the target)\n",
    "slant_range = [(rank * PRI + t) * c / 2 for t in fast_time]\n",
    "\n",
    "# Axes - defines the frequency axes in each direction after FFT\n",
    "SWL = len_range_line / range_sample_freq\n",
    "az_freq_vals = np.arange(-az_sample_freq / 2, az_sample_freq / 2, 1 / (PRI * len_az_line))\n",
    "range_freq_vals = np.arange(-range_sample_freq / 2, range_sample_freq / 2, 1 / SWL)\n",
    "\n",
    "\n",
    "# We need two parameters which vary over range and azimuth, so we're going to loop over these once\n",
    "# D is the cosine of the instantaneous squint angle and is defined by the letter D in most literature\n",
    "# Define a function to calculate D, then apply it inside the loop\n",
    "def d(range_freq, velocity):\n",
    "    return math.sqrt(1 - ((wavelength ** 2 * range_freq ** 2) / (4 * velocity ** 2)))\n",
    "\n",
    "\n",
    "D = np.zeros((len_az_line, len_range_line))\n",
    "\n",
    "# Spacecraft velocity - numerical calculation of the effective spacecraft velocity\n",
    "ecef_vels = ephemeris.apply(lambda x: math.sqrt(\n",
    "    x[\"X-axis velocity ECEF\"] ** 2 + x[\"Y-axis velocity ECEF\"] ** 2 + x[\"Z-axis velocity ECEF\"] ** 2), axis=1)\n",
    "velocity_interp = interp1d(ephemeris[\"POD Solution Data Timestamp\"].unique(), ecef_vels.unique(),\n",
    "                           fill_value=\"extrapolate\")\n",
    "x_interp = interp1d(ephemeris[\"POD Solution Data Timestamp\"].unique(), ephemeris[\"X-axis position ECEF\"].unique(),\n",
    "                    fill_value=\"extrapolate\")\n",
    "y_interp = interp1d(ephemeris[\"POD Solution Data Timestamp\"].unique(), ephemeris[\"Y-axis position ECEF\"].unique(),\n",
    "                    fill_value=\"extrapolate\")\n",
    "z_interp = interp1d(ephemeris[\"POD Solution Data Timestamp\"].unique(), ephemeris[\"Z-axis position ECEF\"].unique(),\n",
    "                    fill_value=\"extrapolate\")\n",
    "space_velocities = selection.apply(lambda x: velocity_interp(x[\"Coarse Time\"] + x[\"Fine Time\"]), axis=1)\n",
    "\n",
    "x_positions = selection.apply(lambda x: x_interp(x[\"Coarse Time\"] + x[\"Fine Time\"]), axis=1).to_list()\n",
    "y_positions = selection.apply(lambda x: y_interp(x[\"Coarse Time\"] + x[\"Fine Time\"]), axis=1).to_list()\n",
    "z_positions = selection.apply(lambda x: z_interp(x[\"Coarse Time\"] + x[\"Fine Time\"]), axis=1).to_list()\n",
    "\n",
    "a = 6378137 # WGS84 semi major axis\n",
    "b = 6356752.3142 # WGS84 semi minor axis\n",
    "velocities = np.zeros((len_az_line, len_range_line))\n",
    "\n",
    "# Now loop over range and azimuth, and calculate spacecraft velocity and D\n",
    "for i in range(len_az_line):\n",
    "    H = math.sqrt(x_positions[i]**2 + y_positions[i]**2 + z_positions[i]**2)\n",
    "    W = float(space_velocities.iloc[i])/H\n",
    "    lat = math.atan(z_positions[i] / x_positions[i])\n",
    "    local_earth_rad = math.sqrt(((a**2 * math.cos(lat))**2 + (b**2 * math.sin(lat))**2) / ((a * math.cos(lat))**2 + (b * math.sin(lat))**2))\n",
    "    for j in range(len_range_line):\n",
    "        cos_beta = (local_earth_rad**2 + H**2 - slant_range[j]**2) / (2 * local_earth_rad * H)\n",
    "        this_ground_velocity = local_earth_rad * W * cos_beta\n",
    "        velocities[i, j] = math.sqrt(float(space_velocities.iloc[i]) * this_ground_velocity)\n",
    "        D[i, j] = d(az_freq_vals[i], velocities[i, j])\n",
    "\n",
    "freq_domain_data = np.zeros((len_az_line, len_range_line), dtype=complex)\n",
    "\n",
    "for az_index in range(len_az_line):\n",
    "    range_line = iq_array[az_index, :]\n",
    "    range_fft = np.fft.fft(range_line)\n",
    "    freq_domain_data[az_index, :] = range_fft\n",
    "\n",
    "for range_index in range(len_range_line):\n",
    "    az_line = freq_domain_data[:, range_index]\n",
    "    az_fft = np.fft.fft(az_line)\n",
    "    az_fft = np.fft.fftshift(az_fft)\n",
    "    freq_domain_data[:, range_index] = az_fft\n",
    "\n",
    "# Create range filter\n",
    "num_tx_vals = int(TXPL*range_sample_freq)\n",
    "tx_replica_time_vals = np.linspace(-TXPL/2, TXPL/2, num=num_tx_vals)\n",
    "phi1 = TXPSF + TXPRR*TXPL/2\n",
    "phi2 = TXPRR/2\n",
    "tx_replica = np.zeros(num_tx_vals, dtype=complex)\n",
    "for i in range(num_tx_vals):\n",
    "    tx_replica[i] = cmath.exp(2j * cmath.pi * (phi1*tx_replica_time_vals[i] + phi2*tx_replica_time_vals[i]**2))\n",
    "\n",
    "range_filter = np.zeros(len_range_line, dtype=complex)\n",
    "index_start = np.ceil((len_range_line-num_tx_vals)/2)-1\n",
    "index_end = num_tx_vals+np.ceil((len_range_line-num_tx_vals)/2)-2\n",
    "range_filter[int(index_start):int(index_end+1)] = tx_replica\n",
    "\n",
    "range_filter = np.fft.fft(range_filter)\n",
    "range_filter = np.conjugate(range_filter)\n",
    "\n",
    "for az_index in range(len_az_line):\n",
    "    freq_domain_data[az_index, :] = freq_domain_data[az_index, :]*range_filter\n",
    "\n",
    "\n",
    "rcmc_filt = np.zeros(len_range_line, dtype=complex)\n",
    "range_freq_vals = np.linspace(-range_sample_freq/2, range_sample_freq/2, num=len_range_line)\n",
    "for az_index in range(len_az_line):\n",
    "    rcmc_filt = np.zeros(len_range_line, dtype=complex)\n",
    "    for range_index in range(len_range_line):\n",
    "        rcmc_shift = slant_range[0]*((1/D[az_index, range_index])-1)\n",
    "        rcmc_filt[range_index] = cmath.exp(4j * cmath.pi * range_freq_vals[range_index] * rcmc_shift / c)\n",
    "    freq_domain_data[az_index, :] = freq_domain_data[az_index, :]*rcmc_filt\n",
    "\n",
    "range_doppler_data = np.zeros((len_az_line, len_range_line), dtype=complex)\n",
    "for range_line_index in range(len_az_line):\n",
    "    ifft = np.fft.ifft(freq_domain_data[range_line_index, :])\n",
    "    ifft_sorted = np.fft.ifftshift(ifft)\n",
    "    range_doppler_data[range_line_index, :] = ifft_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create azimuth filter\n",
    "az_compressed_data = np.zeros((len_az_line, len_range_line), 'complex')\n",
    "\n",
    "for az_line_index in range(len_range_line):\n",
    "    d_vector = np.zeros(len_az_line)\n",
    "    this_az_filter = np.zeros(len_az_line, 'complex')\n",
    "    print('Azimuth_Filter_shape:',this_az_filter.shape)\n",
    "    print(len(slant_range))\n",
    "    print(D.shape)\n",
    "    print(az_freq_vals.shape)\n",
    "    print('Azimuth Freq Vals:',len(az_freq_vals)-1)\n",
    "    for i in range(len(az_freq_vals)-1):  # -1\n",
    "        this_az_filter[i] = cmath.exp((4j * cmath.pi * slant_range[i] * D[i, az_line_index]) / wavelength)\n",
    "    result = range_doppler_data[:, az_line_index] * this_az_filter[:]\n",
    "    result = np.fft.ifft(result)\n",
    "    az_compressed_data[:, az_line_index] = result\n",
    "\n",
    "# Plot final image\n",
    "plt.figure(figsize=(16,100))\n",
    "plt.title(\"Sentinel-1 Processed SAR Image\")\n",
    "plt.imshow(abs(az_compressed_data[:,:]), vmin=0, vmax=2000, origin='lower')\n",
    "plt.xlabel(\"Down Range (samples)\")\n",
    "plt.ylabel(\"Cross Range (samples)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarglasses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
